{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# Detect encoding first\n",
    "with open(\"Career_Decision_Dataset.csv\", 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "\n",
    "# load the dataset using the detected encoding\n",
    "df = pd.read_csv(\"Career_Decision_Dataset.csv\", encoding=result['encoding'])\n",
    "\n",
    "# Check the loaded data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prompt'] = \"Input Prompt: \" + df['Input Prompt'] + \"\\nOutput Scenario: \" + df['Output Scenario']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df[['prompt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-Neo and tokenizer\n",
    "model_name = \"EleutherAI/gpt-neo-125m\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['prompt'], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator for Language Modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=r\"D:\\Career_Decision-bot\\career_GPT_advisor_chatbot_125m_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,  \n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    fp16=False,\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Fine-Tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path on your D: drive\n",
    "output_dir = r\"D:\\Career_Decision-bot\\career_GPT_advisor_chatbot_125m_model\"\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "#model = GPTNeoForCausalLM.from_pretrained(r\"E:\\InterviewGenie\\interview_genie_125m_model\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the selected device (GPU or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Example prompt\n",
    "input_prompt = input(\"Enter your question: \")\n",
    "# Tokenize the input prompt and move the input tensors to the same device as the model\n",
    "inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text based on the prompt\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'],            # Input token IDs\n",
    "    max_length=200,                 # Maximum length of the generated sequence (increase if needed)\n",
    "    num_beams=5,                    # Number of beams for beam search (higher gives better results)\n",
    "    no_repeat_ngram_size=2,         # Prevent repetition of n-grams\n",
    "    temperature=0.5,                 # Lower temperature for more deterministic results\n",
    "    top_p=0.9,                      # Top-p sampling for more controlled randomness\n",
    "    pad_token_id=tokenizer.eos_token_id  # Ensure padding uses EOS token\n",
    ")\n",
    "\n",
    "# Print the generated token IDs before decoding (for debugging)\n",
    "#print(f\"Generated Token IDs: {outputs}\")\n",
    "\n",
    "# Decode the generated tokens back into text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "#answer_start = generated_text.lower().find(\"Output Scenario:\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
